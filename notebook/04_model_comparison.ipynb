{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c397ba7c",
   "metadata": {},
   "source": [
    "# ðŸ“Š Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d43fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross-validated scores\n",
    "print(\"Cross-validated scores:\", scores)\n",
    "\n",
    "# Print the mean R-squared score\n",
    "print(\"Mean cross-validated score: \", scores.mean())\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2_2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Time taken: {time_taken_2} seconds\")\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2) Score:\", r2_2)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "n = len(y)  # number of observations\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Create residual plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Random Forest on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=1, max_features=1.0)\n",
    "\n",
    "# Plotting learning curve for RandomForestRegressor\n",
    "title = \"Learning Curves (RandomForestRegressor)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(rf, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [0.5, 1.0, 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(low=100, high=500),\n",
    "    'max_depth': randint(low=5, high=20),\n",
    "    'min_samples_leaf': randint(low=1, high=4),\n",
    "    'max_features': [0.5, 1.0, 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the random search model\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(random_search.best_params_)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Already defined and trained linear regression model\n",
    "y_test_pred_lr = model.predict(X_test)\n",
    "test_error_lr = mean_squared_error(y_test, y_test_pred_lr)\n",
    "print(f\"Test error of Linear Regression: {test_error_lr}\")\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate validation error of RandomForestRegressor\n",
    "y_test_pred_rf = model_rf.predict(X_test)\n",
    "test_error_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "print(f\"Test error of Random Forest: {test_error_rf}\")\n",
    "\n",
    "# Compare and select the model with lowest validation error\n",
    "best_model = 'Linear Regression' if test_error_lr < test_error_rf else 'Random Forest'\n",
    "print(f\"Best model based on test error is: {best_model}\")\n",
    "\n",
    "\n",
    "# ### *Gradient Boosting*\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8)  # Adjust the number of estimators and learning rate if needed\n",
    "start_time = time.time()\n",
    "gb.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "y_test_pred = gb.predict(X_test)\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "cv_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "# Print cross-validated scores\n",
    "print(\"Cross-validated scores:\", cv_scores)\n",
    "\n",
    "# Print the mean cross-validated score\n",
    "print(\"Mean cross-validated score:\", cv_scores.mean())\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Gradient Boosting on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8)\n",
    "\n",
    "# Plotting learning curve for GradientBoostingRegressor\n",
    "title = \"Learning Curves (GradientBoostingRegressor)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(gb, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor object\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "\n",
    "# ### *Support Vector Regression*\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train the SVR model\n",
    "svm = SVR(kernel='rbf', C=10, epsilon=0.1)\n",
    "start_time = time.time()\n",
    "svm.fit(X_train, y_train)  \n",
    "end_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for SVR on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Define function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Plotting learning curve for your SVR model\n",
    "title = \"Learning Curves (SVR)\"\n",
    "cv = 10  # Define the number of folds for cross-validation, for instance, 10-fold\n",
    "plot_learning_curve(svm, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create a scorer\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring=scorer)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Fit the model with the best parameters to the training data\n",
    "best_svr = SVR(**best_params)\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_svr.predict(X_val)\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_val)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_val)\n",
    "print(\"Validation R-squared:\", r2_val)\n",
    "print(\"Validation Mean Absolute Error (MAE):\", mae_val)\n",
    "\n",
    "\n",
    "# ## Neural Network\n",
    "\n",
    "# ### *Multi-Layer Perceptron*\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% of data for training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 15% for validation, 15% for testing\n",
    "\n",
    "# Create and train the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50, 50, 25), max_iter=1000, batch_size=250, activation='relu', solver='adam', alpha=0.01, early_stopping=True, n_iter_no_change=25, learning_rate='adaptive')\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=25)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# Print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", scores.mean())\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {time_taken} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for MLP on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # Negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # Negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50, 50, 25), max_iter=1000, batch_size=250, activation='relu', solver='adam', alpha=0.01, early_stopping=True, n_iter_no_change=25, learning_rate='adaptive')\n",
    "\n",
    "# Plotting learning curve for MLPRegressor\n",
    "title = \"Learning Curves (MLPRegressor)\"\n",
    "cv = 5  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(model, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50, 25), (100, 50, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Create the MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=1000, early_stopping=True, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50, 50, 25), (100, 50, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(max_iter=1000, early_stopping=True, random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='r2', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_val_pred = random_search.best_estimator_.predict(X_val)\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(100, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),  # Example of dropout layer with 20% dropout rate\n",
    "    Dense(50, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='tanh'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model using the training set\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=150, validation_data=(X_val, y_val))\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val).flatten()\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_val)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_val)\n",
    "print(\"Validation R-squared:\", r2_val)\n",
    "print(\"Validation Mean Absolute Error (MAE):\", mae_val)\n",
    "\n",
    "# ... (continue with the residuals plot)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
