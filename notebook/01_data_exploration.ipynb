{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b082d128",
   "metadata": {},
   "source": [
    "# ðŸ§ª Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b79df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Exploratory Data Analysis \n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "#Load the data\n",
    "df = pd.read_csv('TRAJETORIAS_DATASET_Environmental_dimension_indicators.csv', quoting=3)\n",
    "df.rename(columns=lambda x: x.replace('\"', ''), inplace=True)\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "df['state_abbrev'] = df['state_abbrev'].apply(lambda x: x.replace('\"', ''))\n",
    "df['tempp'] = df['tempp'].apply(lambda x: x.replace('\"', ''))\n",
    "df\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "df = df.drop('state_abbrev', axis=1)\n",
    "df = df.drop('pasture', axis=1)\n",
    "df['tempp'] = df['tempp'].astype('float')\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "#Viewing the data \n",
    "df.head()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "df.describe()\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#Duplicate values\n",
    "df.duplicated().sum()\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "#Finding null values \n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#Finding unique values \n",
    "df['refor'].unique()\n",
    "df['edge'].unique()\n",
    "df['port'].unique()\n",
    "df['river'].unique()\n",
    "df['dgfor'].unique()\n",
    "df['defor'].unique()\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "sns.boxplot(df)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "# Delete rows using drop()\n",
    "df.drop(df[df['edge'] >= 120].index, inplace = True)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "df=df.fillna(0)\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Specify the columns to scale\n",
    "columns_to_scale = ['geocode', 'period']\n",
    "\n",
    "# Scale the selected columns\n",
    "df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df)\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "sns.boxplot(df)\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "import math\n",
    "df['edge'] = df['edge'].apply(lambda x: math.log(x+1))\n",
    "df.drop(df[df['tempp'] >= 15].index, inplace = True)\n",
    "df.drop(df[df['road'] >= 12].index, inplace = True)\n",
    "sns.boxplot(df)\n",
    "print(df)\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(df['tempp'], kde=True)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "#Substituting non-skewed data column with mean\n",
    "mean_value = df['refor'].mean()\n",
    "df['refor'] = df['refor'].fillna(mean_value)\n",
    "\n",
    "#Substituting skewed data columns with median\n",
    "median_value = df[['edge','port','river','dgorg','dgfor','defor','deorg']].median()\n",
    "df[['edge','port','river','dgorg','dgfor','defor','deorg']] = df[['edge','port','river','dgorg','dgfor','defor','deorg']].fillna(median_value)\n",
    "\n",
    "df\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "df.corr()\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#Correlation heatmap\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "#Dropping columns with high correlation\n",
    "\n",
    "df = df.drop('deorg', axis=1)\n",
    "df = df.drop('dgorg', axis=1)\n",
    "df = df.drop('core', axis=1)\n",
    "\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "#Separating categorical and numerical variables \n",
    "cat_cols=df.select_dtypes(include=['object']).columns\n",
    "num_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "print(\"Categorical Variables:\")\n",
    "print(cat_cols)\n",
    "print(\"Numerical Variables:\")\n",
    "print(num_cols)\n",
    "\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "#Univariate analysis for categorical variables\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18))\n",
    "fig.suptitle('Bar plot for all categorical variables in the dataset')\n",
    "\n",
    "sns.countplot(ax=axes[0, 0], x='state', data=df, color='blue',\n",
    "              order=df['state'].value_counts().index)\n",
    "sns.countplot(ax=axes[0, 1], x='municipality', data=df, color='blue',\n",
    "              order=df['municipality'].value_counts().index)\n",
    "\n",
    "axes[0][0].tick_params(labelrotation=45);\n",
    "axes[0][1].tick_params(labelrotation=90);\n",
    "\n",
    "\n",
    "# In[22]:\n",
    "\n",
    "\n",
    "#Relationship between Categorical variables and continuous variables\n",
    "fig, axarr = plt.subplots(4, 2, figsize=(12, 18))\n",
    "df.groupby('refor')['fire'].mean().sort_values(ascending=False).plot.bar(ax=axarr[0][0], fontsize=12)\n",
    "axarr[0][0].set_title(\"Refor vs Fire\", fontsize=18)\n",
    "df.groupby('refor')['tempp'].mean().sort_values(ascending=False).plot.bar(ax=axarr[0][1], fontsize=12)\n",
    "axarr[0][1].set_title(\"Refor vs Tempp\", fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(hspace=1.0)\n",
    "plt.subplots_adjust(wspace=.5)\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "#Multivariate analysis\n",
    "plt.figure(figsize=(12, 7))\n",
    "#sns.heatmap(df.drop(['geocode','period','secveg','core','edge','port','dgfor','defor','precp','precn','tempp'],axis=1).corr(), annot = True, vmin = -1, vmax = 1)\n",
    "sns.heatmap(df.drop(['geocode','period','secveg','edge'],axis=1).corr(), annot = True, vmin = -1, vmax = 1)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[24]:\n",
    "\n",
    "\n",
    "features=df.drop(['geocode','period','secveg','edge','port','state','municipality','refor'],axis=1)\n",
    "for i in features:\n",
    "    sns.lmplot(x=i, y=\"refor\", data=df,line_kws={'color': 'red'})\n",
    "    text=\"Relation between refor and \" + i \n",
    "    plt.title(text)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting a bar plot\n",
    "plt.figure(figsize=(15, 7))\n",
    "sns.barplot(x='state', y='refor', data=df)\n",
    "\n",
    "plt.title(\"Average 'refor' by State\")\n",
    "plt.ylabel('refor')\n",
    "plt.xlabel('State')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ## Machine Learning Models\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "X = df[['fire','secveg','crop','urban','port','river','road','mining','dgfor','defor','precp','precn','tempp']]\n",
    "y = df['refor']\n",
    "\n",
    "\n",
    "# ### *Linear Regression*\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "  # Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=45)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=45)\n",
    "\n",
    "  # Create and train the model\n",
    "model = LinearRegression()\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "time_taken_1 = end_time - start_time\n",
    "\n",
    "  # Predict on the validation set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "import sys\n",
    "\n",
    "# Determine memory usage of objects\n",
    "X_train_size = sys.getsizeof(X_train)\n",
    "X_val_size = sys.getsizeof(X_val)\n",
    "X_test_size = sys.getsizeof(X_test)\n",
    "y_train_size = sys.getsizeof(y_train)\n",
    "y_val_size = sys.getsizeof(y_val)\n",
    "y_test_size = sys.getsizeof(y_test)\n",
    "\n",
    "y_test_pred_size = sys.getsizeof(y_test_pred)\n",
    "total_size = X_train_size + X_val_size + X_test_size + y_train_size + y_val_size + y_test_size + y_test_pred_size\n",
    "model_size = sys.getsizeof(model)\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {time_taken_1} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "print(\"Basic Data Storage:\", total_size)\n",
    "print(\"Model Memory:\", model_size)\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Linear Regression on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Apply Information Gain\n",
    "ig = mutual_info_regression(X, y)\n",
    "\n",
    "# Create a dictionary of feature importance scores\n",
    "feature_scores = {}\n",
    "for i in range(len(X.columns)):\n",
    "    feature_scores[X.columns[i]] = ig[i]\n",
    "\n",
    "# Sort the features by importance score in descending order\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the feature importance scores and the sorted features\n",
    "for feature, score in sorted_features:\n",
    "    print(\"Feature:\", feature, \"Score:\", score)\n",
    "\n",
    "# Plot a horizontal bar chart of the feature importance scores\n",
    "fig, ax = plt.subplots()\n",
    "y_pos = np.arange(len(sorted_features))\n",
    "ax.barh(y_pos, [score for feature, score in sorted_features], align=\"center\")\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([feature for feature, score in sorted_features])\n",
    "ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "ax.set_xlabel(\"Importance Score\")\n",
    "ax.set_title(\"Feature Importance Scores (Information Gain)\")\n",
    "\n",
    "# Add importance scores as labels on the horizontal bar chart\n",
    "for i, v in enumerate([score for feature, score in sorted_features]):\n",
    "    ax.text(v + 0.01, i, str(round(v, 3)), color=\"black\", fontweight=\"bold\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[30]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Plotting learning curve for LinearRegression\n",
    "title = \"Learning Curves (LinearRegression)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(model, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Create and train the Ridge regression model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set using the Ridge regression model\n",
    "y_test_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics on the validation set for the Ridge regression model\n",
    "mse_test_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "rmse_test_ridge = mean_squared_error(y_test, y_test_pred_ridge, squared=False)\n",
    "r2_test_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(\"Ridge Validation Mean Squared Error (MSE):\", mse_test_ridge)\n",
    "print(\"Ridge Validation Root Mean Squared Error (RMSE):\", rmse_test_ridge)\n",
    "print(\"Ridge Validation R-squared:\", r2_test_ridge)\n",
    "\n",
    "# Create and train the Lasso regression model\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set using the Lasso regression model\n",
    "y_test_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Calculate metrics on the validation set for the Lasso regression model\n",
    "mse_test_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "rmse_test_lasso = mean_squared_error(y_test, y_test_pred_lasso, squared=False)\n",
    "r2_test_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(\"Lasso Validation Mean Squared Error (MSE):\", mse_test_lasso)\n",
    "print(\"Lasso Validation Root Mean Squared Error (RMSE):\", rmse_test_lasso)\n",
    "print(\"Lasso Validation R-squared:\", r2_test_lasso)\n",
    "\n",
    "\n",
    "# ### *Decision Tree*\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "dt = DecisionTreeRegressor(max_depth=10, min_samples_leaf=10, min_samples_split=40)  # Adjust the max_depth parameter if needed\n",
    "start_time = time.time()\n",
    "dt.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(dt, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "\n",
    "# Print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Decision Tree on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "dt = DecisionTreeRegressor(max_depth=10, min_samples_leaf=10, min_samples_split=40)\n",
    "\n",
    "# Plotting learning curve for DecisionTreeRegressor\n",
    "title = \"Learning Curves (DecisionTreeRegressor)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(dt, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and the values you want to try\n",
    "param_grid = {\n",
    "    'max_depth': [5, 10, 15, 20, 25],\n",
    "    'min_samples_leaf': [5, 10, 15, 20],\n",
    "    'min_samples_split': [10, 20, 30, 40]\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the model to the data and find the best hyperparameters\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters it found\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the hyperparameters and the distributions to sample from\n",
    "param_dist = {\n",
    "    'max_depth': randint(5, 25),\n",
    "    'min_samples_leaf': randint(5, 20),\n",
    "    'min_samples_split': randint(10, 40)\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(dt, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the model to the data and find the best hyperparameters\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters it found\n",
    "print(random_search.best_params_)\n",
    "\n",
    "\n",
    "# ### *Random Forest*\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=1, max_features=1.0)  # Adjust the number of estimators if needed\n",
    "start_time = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X.columns[indices]\n",
    "sorted_importances = importances[indices]\n",
    "for i in range(len(sorted_importances)):\n",
    "    print(f\"{feature_names[i]}: {sorted_importances[i]}\")\n",
    "    \n",
    "time_taken_2 = end_time - start_time\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "scores = cross_val_score(rf, X_train, y_train, cv=25, scoring='r2')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Make predictions using the Random Forest model\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# Print cross-validated scores\n",
    "print(\"Cross-validated scores:\", scores)\n",
    "\n",
    "# Print the mean R-squared score\n",
    "print(\"Mean cross-validated score: \", scores.mean())\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2_2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Time taken: {time_taken_2} seconds\")\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2) Score:\", r2_2)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "n = len(y)  # number of observations\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Create residual plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Random Forest on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[37]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=10, min_samples_leaf=1, max_features=1.0)\n",
    "\n",
    "# Plotting learning curve for RandomForestRegressor\n",
    "title = \"Learning Curves (RandomForestRegressor)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(rf, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[38]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [0.5, 1.0, 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(low=100, high=500),\n",
    "    'max_depth': randint(low=5, high=20),\n",
    "    'min_samples_leaf': randint(low=1, high=4),\n",
    "    'max_features': [0.5, 1.0, 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Instantiate the random search model\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, \n",
    "                                   n_iter=100, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(random_search.best_params_)\n",
    "\n",
    "\n",
    "# In[40]:\n",
    "\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Already defined and trained linear regression model\n",
    "y_test_pred_lr = model.predict(X_test)\n",
    "test_error_lr = mean_squared_error(y_test, y_test_pred_lr)\n",
    "print(f\"Test error of Linear Regression: {test_error_lr}\")\n",
    "\n",
    "# Train a RandomForestRegressor\n",
    "model_rf = RandomForestRegressor(random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate validation error of RandomForestRegressor\n",
    "y_test_pred_rf = model_rf.predict(X_test)\n",
    "test_error_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "print(f\"Test error of Random Forest: {test_error_rf}\")\n",
    "\n",
    "# Compare and select the model with lowest validation error\n",
    "best_model = 'Linear Regression' if test_error_lr < test_error_rf else 'Random Forest'\n",
    "print(f\"Best model based on test error is: {best_model}\")\n",
    "\n",
    "\n",
    "# ### *Gradient Boosting*\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8)  # Adjust the number of estimators and learning rate if needed\n",
    "start_time = time.time()\n",
    "gb.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "y_test_pred = gb.predict(X_test)\n",
    "\n",
    "# Perform 5-fold cross validation\n",
    "cv_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='r2')\n",
    "\n",
    "# Print cross-validated scores\n",
    "print(\"Cross-validated scores:\", cv_scores)\n",
    "\n",
    "# Print the mean cross-validated score\n",
    "print(\"Mean cross-validated score:\", cv_scores.mean())\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for Gradient Boosting on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # We negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "gb = GradientBoostingRegressor(n_estimators=500, learning_rate=0.05, max_depth=7, subsample=0.8)\n",
    "\n",
    "# Plotting learning curve for GradientBoostingRegressor\n",
    "title = \"Learning Curves (GradientBoostingRegressor)\"\n",
    "cv = 10  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(gb, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor object\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the optimal parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "\n",
    "# ### *Support Vector Regression*\n",
    "\n",
    "# In[42]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "get_ipython().run_line_magic('load_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create and train the SVR model\n",
    "svm = SVR(kernel='rbf', C=10, epsilon=0.1)\n",
    "start_time = time.time()\n",
    "svm.fit(X_train, y_train)  \n",
    "end_time = time.time()\n",
    "\n",
    "# Predict on the validation set\n",
    "y_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {end_time - start_time} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for SVR on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Define function to plot learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Plotting learning curve for your SVR model\n",
    "title = \"Learning Curves (SVR)\"\n",
    "cv = 10  # Define the number of folds for cross-validation, for instance, 10-fold\n",
    "plot_learning_curve(svm, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'epsilon': [0.01, 0.1, 1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create a scorer\n",
    "scorer = make_scorer(r2_score)\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(SVR(), param_grid, cv=5, scoring=scorer)\n",
    "\n",
    "# Fit the model to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Fit the model with the best parameters to the training data\n",
    "best_svr = SVR(**best_params)\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = best_svr.predict(X_val)\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_val)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_val)\n",
    "print(\"Validation R-squared:\", r2_val)\n",
    "print(\"Validation Mean Absolute Error (MAE):\", mae_val)\n",
    "\n",
    "\n",
    "# ## Neural Network\n",
    "\n",
    "# ### *Multi-Layer Perceptron*\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import explained_variance_score\n",
    "\n",
    "get_ipython().run_line_magic('reload_ext', 'memory_profiler')\n",
    "\n",
    "get_ipython().run_line_magic('memit', '')\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% of data for training\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 15% for validation, 15% for testing\n",
    "\n",
    "# Create and train the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50, 50, 25), max_iter=1000, batch_size=250, activation='relu', solver='adam', alpha=0.01, early_stopping=True, n_iter_no_change=25, learning_rate='adaptive')\n",
    "start_time = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "time_taken = end_time - start_time\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=25)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "\n",
    "# Print the mean cross-validation score\n",
    "print(\"Mean cross-validation score:\", scores.mean())\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(f\"Time taken: {time_taken} seconds\")\n",
    "print(\"Test Mean Squared Error (MSE):\", mse_test)\n",
    "print(\"Test Root Mean Squared Error (RMSE):\", rmse_test)\n",
    "print(\"Test R-squared:\", r2_test)\n",
    "print(\"Test Mean Absolute Error (MAE):\", mae_test)\n",
    "\n",
    "# Calculate residuals for the validation set\n",
    "residuals_test = y_test - y_test_pred\n",
    "\n",
    "# Create residual plot for the validation set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test_pred, residuals_test, alpha=0.5)\n",
    "plt.axhline(y=0, color='red')\n",
    "plt.title('Residuals vs Predicted Values for MLP on Test Set')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)  # Negate because learning_curve returns negative values for MSE\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -np.mean(test_scores, axis=1)  # Negate because learning_curve returns negative values for MSE\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "\n",
    "# Defining the model\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50, 50, 25), max_iter=1000, batch_size=250, activation='relu', solver='adam', alpha=0.01, early_stopping=True, n_iter_no_change=25, learning_rate='adaptive')\n",
    "\n",
    "# Plotting learning curve for MLPRegressor\n",
    "title = \"Learning Curves (MLPRegressor)\"\n",
    "cv = 5  # Define the number of folds for cross-validation\n",
    "plot_learning_curve(model, title, X_train, y_train, cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[43]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50, 50, 25), (100, 50, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "# Create the MLPRegressor object\n",
    "mlp = MLPRegressor(max_iter=1000, early_stopping=True, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='r2', n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_val_pred = grid_search.best_estimator_.predict(X_val)\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'hidden_layer_sizes': [(50, 50, 25), (100, 50, 50, 25)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "mlp = MLPRegressor(max_iter=1000, early_stopping=True, random_state=42)\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=5, scoring='r2', n_jobs=-1, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"Best hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict on the validation set using the best model\n",
    "y_val_pred = random_search.best_estimator_.predict(X_val)\n",
    "\n",
    "\n",
    "# In[97]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(100, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),  # Example of dropout layer with 20% dropout rate\n",
    "    Dense(50, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(50, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='tanh'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model using the training set\n",
    "model.fit(X_train, y_train, batch_size=50, epochs=150, validation_data=(X_val, y_val))\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_val_pred = model.predict(X_val).flatten()\n",
    "\n",
    "# Calculate metrics on the validation set\n",
    "mse_val = mean_squared_error(y_val, y_val_pred)\n",
    "rmse_val = np.sqrt(mse_val)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse_val)\n",
    "print(\"Validation Root Mean Squared Error (RMSE):\", rmse_val)\n",
    "print(\"Validation R-squared:\", r2_val)\n",
    "print(\"Validation Mean Absolute Error (MAE):\", mae_val)\n",
    "\n",
    "# ... (continue with the residuals plot)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
